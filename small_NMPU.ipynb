{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"small_NMPU.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"br_WUTuNsY_o","colab_type":"code","outputId":"93be0025-5d65-4266-9ae4-5d5996588731","executionInfo":{"status":"ok","timestamp":1569132530440,"user_tz":-480,"elapsed":18040,"user":{"displayName":"LIM CAI XIAN LUCINDA","photoUrl":"","userId":"12327338794304454977"}},"colab":{"base_uri":"https://localhost:8080/","height":106}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3ldyEdo4sNa_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cF5zbZYyLI4c","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RragX8fbOV6t","colab_type":"code","outputId":"06c47321-88ae-420c-f19a-a71146cd05cb","executionInfo":{"status":"ok","timestamp":1569118435474,"user_tz":-480,"elapsed":997,"user":{"displayName":"CHAEMCHOY PHATCHARAWAT","photoUrl":"","userId":"08374501170702879148"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["pwd"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"G-SNrrZiOWDt","colab_type":"code","outputId":"63a510f4-e9b5-4f13-8e6b-560329a7dc49","executionInfo":{"status":"ok","timestamp":1569132544405,"user_tz":-480,"elapsed":11196,"user":{"displayName":"LIM CAI XIAN LUCINDA","photoUrl":"","userId":"12327338794304454977"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","total_len = 0\n","for i in os.listdir('/content/drive/My Drive/VipAssign/train'):\n","  total_len += len(os.listdir('/content/drive/My Drive/VipAssign/train/'+i))\n","\n","print(total_len)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["6627\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x6n1D7DksJnl","colab_type":"code","outputId":"9c7e1136-ba23-4d44-c778-77a892911b78","executionInfo":{"status":"ok","timestamp":1569118365609,"user_tz":-480,"elapsed":7995,"user":{"displayName":"CHAEMCHOY PHATCHARAWAT","photoUrl":"","userId":"08374501170702879148"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["ls \"/content/drive/My Drive/VipAssign/test\""],"execution_count":0,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbeach\u001b[0m/            \u001b[01;34mdesert\u001b[0m/    \u001b[01;34mforest\u001b[0m/           \u001b[01;34misland\u001b[0m/  \u001b[01;34mmountain\u001b[0m/     \u001b[01;34mriver\u001b[0m/\n","\u001b[01;34mcommercial_area\u001b[0m/  \u001b[01;34mfarmland\u001b[0m/  \u001b[01;34mindustrial_area\u001b[0m/  \u001b[01;34mlake\u001b[0m/    \u001b[01;34mresidential\u001b[0m/  \u001b[01;34mwetland\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qB7yCO7etGLs","colab_type":"code","outputId":"ea16890d-177b-445b-c2b3-868f88513ea7","executionInfo":{"status":"ok","timestamp":1569081415401,"user_tz":-480,"elapsed":4690,"user":{"displayName":"CHAEMCHOY PHATCHARAWAT","photoUrl":"","userId":"08374501170702879148"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["ls 'bottleneck_fc_model.h5'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["ls: cannot access 'bottleneck_fc_model.h5': No such file or directory\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yXzJ3gT0tyxz","colab_type":"code","outputId":"77b5a184-6ce3-499b-c14c-1e53964edb78","executionInfo":{"status":"ok","timestamp":1569138157766,"user_tz":-480,"elapsed":5615681,"user":{"displayName":"LIM CAI XIAN LUCINDA","photoUrl":"","userId":"12327338794304454977"}},"colab":{"base_uri":"https://localhost:8080/","height":331}},"source":["# save_bottlebeck feature\n","import numpy as np\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.models import Sequential\n","from keras.layers import Dropout, Flatten, Dense\n","from keras import applications\n","\n","#solve keras is not defined issue\n","from keras import utils as np_utils\n","import keras\n","\n","# dimensions of our images.\n","img_width, img_height = 256, 256\n","\n","top_model_weights_path = \"/content/drive/My Drive/VipAssign/bottleneck_fc_model.h5\"\n","train_data_dir         = \"/content/drive/My Drive/VipAssign/train\"\n","validation_data_dir    = \"/content/drive/My Drive/VipAssign/test\"\n","\n","nb_train_samples = 6600\n","nb_validation_samples = 1800\n","epochs = 50\n","batch_size = 20\n","\n","\n","def save_bottlebeck_features():\n","    datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","    # build the VGG16 network\n","    model = applications.VGG16(include_top=False, weights='imagenet')\n","    \n","    generator = datagen.flow_from_directory(\n","        validation_data_dir,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        #(own)-change from None to Categorical\n","        class_mode='categorical',\n","        shuffle=False)\n","    bottleneck_features_validation = model.predict_generator(\n","        generator, nb_validation_samples // batch_size)\n","    np.save('bottleneck_features_validation.npy',bottleneck_features_validation)\n","    nb_validation_labels=generator.classes\n","\n","    generator = datagen.flow_from_directory(\n","        train_data_dir,\n","        target_size=(img_width, img_height),\n","        batch_size=batch_size,\n","        #(own)-change from None to Categorical\n","        class_mode='categorical',\n","        shuffle=False)\n","    bottleneck_features_train = model.predict_generator(\n","        generator, nb_train_samples // batch_size)\n","    np.save('bottleneck_features_train.npy',bottleneck_features_train)    \n","    nb_train_labels=generator.classes\n","    \n","    \n","   \n","    \n","    \n","    return(nb_validation_labels,nb_train_labels)\n","\n"," \n","  \n","  \n","nb_validation_labels,nb_train_labels=save_bottlebeck_features()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58892288/58889256 [==============================] - 1s 0us/step\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","Found 1800 images belonging to 12 classes.\n","Found 6627 images belonging to 12 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XByJ9SXayYaU","colab_type":"code","outputId":"f7bb3617-425d-4d51-c994-bfa532a95c58","executionInfo":{"status":"ok","timestamp":1569138157777,"user_tz":-480,"elapsed":5612482,"user":{"displayName":"LIM CAI XIAN LUCINDA","photoUrl":"","userId":"12327338794304454977"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["print(nb_train_samples)\n","print(nb_train_labels.size)\n","print(nb_validation_samples)\n","print(nb_validation_labels)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["6600\n","6627\n","1800\n","[ 0  0  0 ... 11 11 11]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"w8DLmtNXydqR","colab_type":"code","outputId":"2c0de266-fccc-41dc-b1bc-5a71c68b2c22","executionInfo":{"status":"error","timestamp":1569138160828,"user_tz":-480,"elapsed":5443049,"user":{"displayName":"LIM CAI XIAN LUCINDA","photoUrl":"","userId":"12327338794304454977"}},"colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["\n","def train_top_model():\n","    train_data = np.load('bottleneck_features_train.npy')\n","    #train_labels = np.array([0] * (nb_train_samples // 8) + [1] * (nb_train_samples // 8)+[2] * (nb_train_samples // 8) + [3] * (nb_train_samples // 8)+[4] * (nb_train_samples // 8) + [5] * (nb_train_samples // 8)+[6] * (nb_train_samples // 8) + [7] * (nb_train_samples // 8))\n","    #(own)\n","    num_classes=12\n","\n","    \n","    train_labels = keras.utils.to_categorical(y=nb_train_labels, num_classes=num_classes)\n","    \n","#     labels[[0,0,1,0],\n","#            [0,0,0,1],\n","#            [0,1,0,0]]\n","    \n","    print(train_labels)\n","    \n","    validation_data = np.load('bottleneck_features_validation.npy')\n","    #validation_labels = np.array([0] * (nb_validation_samples // 8) + [1] * (nb_validation_samples // 8)+[2] * (nb_validation_samples // 8) + [3] * (nb_validation_samples // 8)+[4] * (nb_validation_samples // 8) + [5] * (nb_validation_samples // 8)+[6] * (nb_validation_samples // 8) + [7] * (nb_validation_samples // 8))\n","    #(own)\n","    validation_labels = keras.utils.to_categorical(nb_validation_labels, num_classes)\n","    \n","    #can use scikit learn to build the layers\n","    model = Sequential()\n","    model.add(Flatten(input_shape=train_data.shape[1:]))\n","    model.add(Dense(256, activation='relu'))\n","    model.add(Dropout(0.5))\n","    #(own)8,softmax\n","    model.add(Dense(12, activation='softmax'))\n","    #categorical\n","    model.compile(optimizer='rmsprop',\n","                  loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    model.fit(train_data, train_labels,\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              validation_data=(validation_data, validation_labels))\n","    model.save_weights(top_model_weights_path)\n","    \n","    \n","    \n","train_top_model()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[[1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]\n"," [0. 0. 0. ... 0. 0. 1.]]\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-3f7641f8b67e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_top_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-3f7641f8b67e>\u001b[0m in \u001b[0;36mtrain_top_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m               validation_data=(validation_data, validation_labels))\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_model_weights_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1089\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m                 \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    238\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         raise ValueError('All sample_weight arrays should have '\n","\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 6600 input samples and 6627 target samples."]}]},{"cell_type":"code","metadata":{"id":"cHjipjP33Pxj","colab_type":"code","colab":{}},"source":["#fine-tuning\n","\n","from keras import applications\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import optimizers\n","from keras.models import Sequential\n","from keras.models import Model\n","from keras.layers import Dropout, Flatten, Dense\n","\n","# path to the model weights files.\n","weights_path = '../keras/examples/vgg16_weights.h5'\n","top_model_weights_path = 'bottleneck_fc_model.h5'\n","\n","# dimensions of our images.\n","img_width, img_height = 256,256\n","\n","train_data_dir=\"/content/drive/My Drive/VipAssign/train\"\n","validation_data_dir=\"/content/drive/My Drive/VipAssign/test\"\n","nb_train_samples =6600\n","nb_validation_samples = 1800\n","epochs = 50\n","batch_size = 20\n","\n","# build the VGG16 network\n","base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=(256,256,3))\n","print('Model loaded.')\n","\n","# build a classifier model to put on top of the convolutional model\n","top_model = Sequential()\n","top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n","top_model.add(Dense(256, activation='relu'))\n","top_model.add(Dropout(0.5))\n","top_model.add(Dense(8, activation='softmax'))\n","\n","# note that it is necessary to start with a fully-trained\n","# classifier, including the top classifier,\n","# in order to successfully do fine-tuning\n","top_model.load_weights(top_model_weights_path)\n","\n","# add the model on top of the convolutional base\n","# model.add(top_model)\n","model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n","\n","# set the first 25 layers (up to the last conv block)\n","# to non-trainable (weights will not be updated)\n","for layer in model.layers[:15]:\n","    layer.trainable = False\n","\n","# compile the model with a SGD/momentum optimizer\n","# and a very slow learning rate.\n","model.compile(loss='categorical_crossentropy',\n","              optimizer=optimizers.SGD(lr=1e-4, momentum=0.9),\n","              metrics=['accuracy'])\n","\n","# prepare data augmentation configuration\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True)\n","\n","test_datagen = ImageDataGenerator(rescale=1. / 255)\n","\n","train_generator = train_datagen.flow_from_directory(\n","    train_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","validation_generator = test_datagen.flow_from_directory(\n","    validation_data_dir,\n","    target_size=(img_height, img_width),\n","    batch_size=batch_size,\n","    class_mode='categorical')\n","\n","model.summary()\n","\n","# fine-tune the model\n","model.fit_generator(\n","    train_generator,\n","    steps_per_epoch=nb_train_samples // batch_size,\n","    epochs=epochs,\n","    validation_data=validation_generator,\n","    validation_steps=nb_validation_samples // batch_size,\n","    verbose=2)"],"execution_count":0,"outputs":[]}]}